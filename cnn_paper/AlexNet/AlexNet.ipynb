{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AlexNet.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPMV8r39XWEvwqCh9wwUNWf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"lnq7e6ON_bFf"},"source":["# 필요 라이브러리 \n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils import data\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"36SU-8_U_3Bs"},"source":["!pip install tensorboardX"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0JhA2fyH_-fT"},"source":["from tensorboardX import SummaryWriter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VF_k3VNNACp5"},"source":["# device => cuda\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HDVmFnqCALdJ"},"source":["print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"obkhVF_vAOvo"},"source":["# model parameters define\n","NUM_EPOCHS = 90\n","BATCH_SIZE = 128\n","MOMENTUM = 0.9\n","LR_DECAY = 0.0005\n","LR_INIT = 0.01\n","IMAGE_DIM = 227 # pixels\n","NUM_CLASSES = 1000\n","DEVICE_IDS = [0, 1, 2, 3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3NpR8EA0As9_"},"source":["from google.colab import drive \n","drive.mount('/content/gdrive/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOqclGuMBCv9"},"source":["# 현재경로\n","print(os.path.abspath(os.getcwd()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nu0Tbry7AjK3"},"source":["# data directory 지정하기\n","INPUT_ROOT_DIR = './gdrive/MyDrive/testdata/alexnet_data_in'\n","TRAIN_IMG_DIR = './gdrive/MyDrive/testdata/alexnet_data_in/imagenet'\n","OUTPUT_DIR = './gdrive/MyDrive/testdata/alexnet_data_out'\n","LOG_DIR = OUTPUT_DIR + '/tblogs'  # tensorboard logs\n","CHECKPOINT_DIR = OUTPUT_DIR + '/models'  # model checkpoints\n","\n","# checkpoint 경로 directory 만들기\n","os.makedirs(INPUT_ROOT_DIR)\n","os.makedirs(TRAIN_IMG_DIR)\n","os.makedirs(OUTPUT_DIR)\n","os.makedirs(LOG_DIR)\n","os.makedirs(CHECKPOINT_DIR, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nawOGUjWBBej"},"source":["# Networks design\n","\n","class AlexNet(nn.Module):\n","    def __init__(self, num_classes = 1000):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(in_channels = 3, out_channels = 96, kernel_size=11, stride=4),\n","            nn.ReLU(),\n","            nn.LocalResponseNorm(size=5, alpha=0.0001, beta= 0.75, k=2),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(96, 256, 5, padding= 2),\n","            nn.ReLU(),\n","            nn.LocalResponseNorm(size= 5, alpha= 0.0001, beta= 0.75, k= 2),\n","            nn.MaxPool2d(kernel_size= 3, stride= 2),\n","            nn.Conv2d(256, 384, 3, padding = 1),\n","            nn.ReLU(),\n","            nn.Conv2d(384, 384, 3, padding = 1),\n","            nn.ReLU(),\n","            nn.Conv2d(384, 256, 3, padding = 1),\n","            nn.ReLU(), \n","            nn.MaxPool2d(kernel_size= 3, stride= 2),\n","        )\n","\n","        # FC layer\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(p=0.5, inplace=True),\n","            nn.Linear(in_features=(256 * 6 * 6), out_features=4096),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.5, inplace=True),\n","            nn.Linear(in_features=4096, out_features=4096),\n","            nn.ReLU(),\n","            nn.Linear(in_features=4096, out_features=num_classes),\n","        )\n","\n","        #bias 초기화\n","        self.init_bias()\n","    def init_bias(self):\n","        for layer in self.net:\n","            if isinstance(layer, nn.Conv2d):\n","                #weight / bias initialze\n","                nn.init.normal_(layer.weight, mean = 0, std= 0.01)\n","                nn.init.constant_(layer.bias, 0)\n","        # 논문에 2, 4, 5 컨볼류션 계층의 bias 1로 초기화 한다고함\n","        nn.init.constant_(self.net[4].bias, 1)\n","        nn.init.constant_(self.net[10].bias, 1)\n","        nn.init.constant_(self.net[12].bias, 1)\n","\n","    def forward(self, x):\n","        x = self.net(x)\n","        x = x.view(-1, 256 * 6 * 6)\n","        return self.classifier(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uW7sQ2znFsSW"},"source":["# 데이터 전처리, 손실함수, optimizer 설정, 학습\n","\n","if __name__ == '__main__':\n","    #seed value 출력하기\n","    seed = torch.initial_seed()\n","    print('Used seed : {}'.format(seed))\n","    tbwrter = SummaryWriter(log_dir= LOG_DIR)\n","    print('TensorboardX summary writer created')\n","\n","    #model 생성하기\n","    alexnet = AlexNet(num_classes= NUM_CLASSES).to(device)\n","    #다수 GPU에서 학습\n","    #alexnet = torch.nn.parallel.DataParallel(alexnet, device_ids=DEVICE_IDS)\n","    print(alexnet)\n","    print('AlexNet created')\n","\n","    #dataset && dataLoader\n","    dataset = datasets.ImageFolder(TRAIN_IMG_DIR, transforms.Compose([\n","        # transforms.RandomResizedCrop(IMAGE_DIM, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n","        transforms.CenterCrop(IMAGE_DIM),\n","        # transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ]))\n","    print('Dataset created')\n","    dataloader = data.DataLoader(\n","        dataset,\n","        shuffle=True,\n","        pin_memory=True,\n","        num_workers=8,\n","        drop_last=True,\n","        batch_size=BATCH_SIZE)\n","    print('Dataloader created')\n","\n","    # optimizer 생성하기\n","    optimizer = optim.SGD(\n","        params=alexnet.parameters(),\n","        lr=LR_INIT,\n","        momentum=MOMENTUM,\n","        weight_decay=LR_DECAY)\n","    print('Optimizer created')\n","    \n","    # lr_scheduler로 LR 감소시키기 : 30epochs 마다 1/10\n","    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n","    print('LR Scheduler created')\n","\n","    # train 시작\n","    print('Starting training...')\n","    total_steps = 1\n","    for epoch in range(NUM_EPOCHS):\n","        lr_scheduler.step()\n","        for imgs, classes in dataloader:\n","            imgs, classes = imgs.to(device), classes.to(device)\n","\n","            # loss 계산\n","            output = alexnet(imgs)\n","            loss = F.cross_entropy(output, classes)\n","\n","            # parameter 갱신\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # log the information and add to tensorboard\n","            # 정보를 기록하고 tensorboard에 추가하기\n","            if total_steps % 10 == 0:\n","                with torch.no_grad():\n","                    _, preds = torch.max(output, 1)\n","                    accuracy = torch.sum(preds == classes)\n","\n","                    print('Epoch: {} \\tStep: {} \\tLoss: {:.4f} \\tAcc: {}'\n","                        .format(epoch + 1, total_steps, loss.item(), accuracy.item()))\n","                    tbwriter.add_scalar('loss', loss.item(), total_steps)\n","                    tbwriter.add_scalar('accuracy', accuracy.item(), total_steps)\n","\n","            # gradient values와 parameter average values 추력하기\n","            if total_steps % 100 == 0:\n","                with torch.no_grad():\n","                    # parameters의 grad 출력하고 저장하기\n","                    # parameters values 출력하고 저장하기\n","                    print('*' * 10)\n","                    for name, parameter in alexnet.named_parameters():\n","                        if parameter.grad is not None:\n","                            avg_grad = torch.mean(parameter.grad)\n","                            print('\\t{} - grad_avg: {}'.format(name, avg_grad))\n","                            tbwriter.add_scalar('grad_avg/{}'.format(name), avg_grad.item(), total_steps)\n","                            tbwriter.add_histogram('grad/{}'.format(name),\n","                                    parameter.grad.cpu().numpy(), total_steps)\n","                        if parameter.data is not None:\n","                            avg_weight = torch.mean(parameter.data)\n","                            print('\\t{} - param_avg: {}'.format(name, avg_weight))\n","                            tbwriter.add_histogram('weight/{}'.format(name),\n","                                    parameter.data.cpu().numpy(), total_steps)\n","                            tbwriter.add_scalar('weight_avg/{}'.format(name), avg_weight.item(), total_steps)\n","\n","            total_steps += 1\n","\n","        # checkpoints 저장하기\n","        checkpoint_path = os.path.join(CHECKPOINT_DIR, 'alexnet_states_e{}.pkl'.format(epoch + 1))\n","        state = {\n","            'epoch': epoch,\n","            'total_steps': total_steps,\n","            'optimizer': optimizer.state_dict(),\n","            'model': alexnet.state_dict(),\n","            'seed': seed,\n","        }\n","        torch.save(state, checkpoint_path)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wIzbqpHiIQLL"},"source":[""],"execution_count":null,"outputs":[]}]}