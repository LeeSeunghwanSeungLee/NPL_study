{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Torchtext_KOR.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPH3JQLlPkDxKRqnxazRzpL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"bs19mzTpTSro"},"source":["# Colab 에 Mecab 설치\r\n","\r\n","!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\r\n","%cd Mecab-ko-for-Google-Colab\r\n","!bash install_mecab-ko_on_colab190912.sh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KykR8EaoTZne"},"source":["import urllib.request\r\n","import pandas as pd\r\n","\r\n","# traing & test set download, save\r\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\r\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTtRLsWkUkwn"},"source":["train_df = pd.read_table('ratings_train.txt')\r\n","test_df = pd.read_table('ratings_test.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jXl_G6e0Uti-"},"source":["train_df.head() # 상위 5개의 정보 & col 정보를 가져온다"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hQ3wbSl9UySv"},"source":["test_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VwdEeaCsU14u"},"source":["print('훈련 데이터 샘플의 개수 : {}'.format(len(train_df)))\r\n","print('테스트 데이터 샘플의 개수 : {}'.format(len(test_df)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ucjt72c0U4zL"},"source":["# 필드 정의하기\r\n","\r\n","from torchtext import data # torchtext.data 임포트\r\n","from konlpy.tag import Mecab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dtKFL6kZVoZ5"},"source":["# Mecab 을 토크나이저로 사용한다\r\n","tokenizer = Mecab()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iLk_O5JfVsMu"},"source":["ID = data.Field(sequential = False,\r\n","                use_vocab = False)\r\n","\r\n","TEXT = data.Field(sequential = True,\r\n","                  use_vocab = True,\r\n","                  tokenize = tokenizer.morphs,\r\n","                  lower = True,\r\n","                  batch_first = True,\r\n","                  fix_length = 20)\r\n","\r\n","LABEL = data.Field(sequential = False,\r\n","                   use_vocab = False,\r\n","                   is_target = True)\r\n","\r\n","# sequential : 시퀀스 데이터 여부. (True가 기본값)\r\n","# use_vocab : 단어 집합을 만들 것인지 여부. (True가 기본값)\r\n","# tokenize : 어떤 토큰화 함수를 사용할 것인지 지정. (string.split이 기본값)\r\n","# lower : 영어 데이터를 전부 소문자화한다. (False가 기본값)\r\n","# batch_first : 미니 배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지 여부. (False가 기본값)\r\n","# is_target : 레이블 데이터 여부. (False가 기본값)\r\n","# fix_length : 최대 허용 길이. 이 길이에 맞춰서 패딩 작업(Padding)이 진행된다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drenMQFyWBfo"},"source":["# 데이터 셋 만들기\r\n","\r\n","from torchtext.data import TabularDataset\r\n","\r\n","train_data, test_data = TabularDataset.splits(\r\n","    path = '.', train = 'ratings_train.txt', test = 'ratings_test.txt', format = 'tsv',\r\n","    fields = [('id',ID), ('text', TEXT), ('label', LABEL)], skip_header = True\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E3FuVgcvWsxI"},"source":["print('훈련 샘플의 개수 : {}'.format(len(train_data)))\r\n","print('테스트 샘플의 개수 : {}'.format(len(test_data)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hw6x0qTHW0Bd"},"source":["print(vars(train_data[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MDVoAMv1W4g3"},"source":["# vocabulary (단어 집합)\r\n","\r\n","TEXT.build_vocab(train_data, min_freq = 10, max_size = 10000)\r\n","print('단어 집합의 크기 : {}'.format(len(TEXT.vocab)))\r\n","\r\n","print(TEXT.vocab.stoi) # iterator 로 표현"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HO7WPqlQXJ9J"},"source":["# 토치 텍스트의 데이터 로더 만들기\r\n","\r\n","from torchtext.data import Iterator\r\n","\r\n","batch_size = 5\r\n","train_loader = Iterator(dataset = train_data, batch_size = batch_size)\r\n","test_loader = Iterator(dataset = test_data, batch_size = batch_size)\r\n","\r\n","print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))\r\n","print('테스트 데이터의 미니 배치 수 : {}'.format(len(test_loader)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wIJCcfd4Xw5A"},"source":["batch = next(iter(train_loader)) # 첫번째 미니배치\r\n","print(batch.text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lhk9vHDZX1Nx"},"source":["batch = next(iter(train_loader)) # 두번째 미니배치\r\n","print(batch.text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4jJHBMjxX9_b"},"source":["# 토치 텍스트에서 batch_first True / False 비교\r\n","\r\n","import urllib.request\r\n","import pandas as pd\r\n","\r\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2OwCG6JIYK1D"},"source":["df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')\r\n","print('전체 샘플의 개수 : {}'.format(len(df)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7CTHZSCIYSl3"},"source":["train_df = df[:25000] # slicing 은 얕은 복사에 해당한다\r\n","test_df = df[25000:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YjoVSQ6LYqJC"},"source":["train_df.to_csv(\"train_data.csv\", index=False)\r\n","test_df.to_csv(\"test_data.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yzqCb_aDYsR5"},"source":["from torchtext import data\r\n","\r\n","TEXT = data.Field(sequential=True,\r\n","                  use_vocab = True,\r\n","                  tokenize = str.split,\r\n","                  lower = True,\r\n","                  batch_first = True, # 미니배치 차원을 맨 앞으로 해서 데이터를 불러올 것인지 여부,\r\n","                  fix_length = 20)\r\n","\r\n","\r\n","LABEL = data.Field(sequential = False,\r\n","                   use_vocab = False,\r\n","                   batch_first = False,\r\n","                   is_target = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IAMSvdL7ZxcJ"},"source":["from torchtext.data import TabularDataset\r\n","from torchtext.data import Iterator\r\n","\r\n","# TabularDataset은 데이터를 불러오면서 필드에서 정의했던 토큰화 방법으로 토큰화를 수행합니다.\r\n","train_data, test_data = TabularDataset.splits(\r\n","        path='.', train='train_data.csv', test='test_data.csv', format='csv',\r\n","        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)\r\n","\r\n","# 정의한 필드에 .build_vocab() 도구를 사용하면 단어 집합을 생성합니다.\r\n","TEXT.build_vocab(train_data, min_freq=10, max_size=10000) # 10,000개의 단어를 가진 단어 집합 생성\r\n","\r\n","# 배치 크기를 정하고 첫번째 배치를 출력해보겠습니다.\r\n","batch_size = 5\r\n","train_loader = Iterator(dataset=train_data, batch_size = batch_size)\r\n","batch = next(iter(train_loader)) # 첫번째 미니배치\r\n","print(batch.text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_8gGcTCaPqE"},"source":["print(batch.text.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iu57XJJ9aSpi"},"source":["# 필드 재정의\r\n","TEXT = data.Field(sequential=True,\r\n","                  use_vocab=True,\r\n","                  tokenize=str.split,\r\n","                  lower=True,\r\n","                  fix_length=20)\r\n","\r\n","LABEL = data.Field(sequential=False,\r\n","                   use_vocab=False,\r\n","                   batch_first=False,\r\n","                   is_target=True)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"56kt1deAaV0A"},"source":["print(batch.text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rd1iI5hXaYoS"},"source":[""],"execution_count":null,"outputs":[]}]}